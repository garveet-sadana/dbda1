Create a Pyspark Streaming program to perform following 
1.Monitor a directory /tmp/property_data for incoming files.
2.As soon as file is detected , ingest it into HDFS 
3.Save the ingested data into parquet format in HDFS
4. Use the provided dataset and compute the following
a.Total no of property in each property type. 
b. Average rent Estimation _currentPrice for each country

Assume the dataset of London House Price Dataset



from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg
from pyspark.streaming import StreamingContext
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("PropertyDataIngestion") \
    .getOrCreate()

# Initialize Spark Streaming context
ssc = StreamingContext(spark.sparkContext, 10)  # 10 seconds window to check for new files

# Define the schema for the incoming data based on the provided dataset
schema = StructType([
    StructField("floorAreaSqM", FloatType(), True),
    StructField("livingRooms", IntegerType(), True),
    StructField("tenure", StringType(), True),
    StructField("propertyType", StringType(), True),
    StructField("currentEnergyRating", StringType(), True),
    StructField("rentEstimate_lowerPrice", FloatType(), True),
    StructField("rentEstimate_currentPrice", FloatType(), True),
    StructField("rentEstimate_upperPrice", FloatType(), True),
    StructField("saleEstimate_lowerPrice", FloatType(), True),
    StructField("saleEstimate_currentPrice", FloatType(), True),
    StructField("saleEstimate_upperPrice", FloatType(), True),
    StructField("saleEstimate_confidenceLevel", StringType(), True),
    StructField("saleEstimate_ingestedAt", StringType(), True),
    StructField("saleEstimate_valueChange.numericChange", FloatType(), True),
    StructField("saleEstimate_valueChange.percentageChange", FloatType(), True),
    StructField("saleEstimate_valueChange.saleDate", DateType(), True),
    StructField("history_date", DateType(), True),
    StructField("history_price", FloatType(), True),
    StructField("history_percentageChange", FloatType(), True),
    StructField("history_numericChange", FloatType(), True),
])

# Directory to monitor for incoming files
directory_path = "/tmp/property_data"

# Function to process each RDD from the stream
def process_rdd(rdd):
    if not rdd.isEmpty():
        # Read the incoming data as a DataFrame
        df = spark.read.csv(rdd, header=True, schema=schema)
        
        # Save the data to HDFS in Parquet format
        df.write.mode("append").parquet("hdfs://namenode_host:8020/property_data_parquet/")
        
        # Compute the total number of properties in each property type
        property_counts = df.groupBy("propertyType").count()
        
        # Compute the average rent estimate currentPrice by property type
        avg_rent_by_property = df.groupBy("propertyType").agg(
            avg("rentEstimate_currentPrice").alias("avg_rentEstimate_currentPrice")
        )
        
        # Show results (for debugging purposes)
        property_counts.show()
        avg_rent_by_property.show()

# Set up the stream to read files
stream = ssc.textFileStream(directory_path)

# Process the incoming data
stream.foreachRDD(process_rdd)

# Start the streaming context
ssc.start()

# Wait for the streaming to finish
ssc.awaitTermination()























floorAreaSqM	livingRooms	tenure	propertyType	currentEnergyRating	rentEstimate_lowerPrice	rentEstimate_currentPrice	rentEstimate_upperPrice	saleEstimate_lowerPrice	saleEstimate_currentPrice	saleEstimate_upperPrice	saleEstimate_confidenceLevel	saleEstimate_ingestedAt	saleEstimate_valueChange.numericChange	saleEstimate_valueChange.percentageChange	saleEstimate_valueChange.saleDate	history_date	history_price	history_percentageChange	history_numericChange
79		Freehold	Flat/Maisonette		2350	2550	2800	444000	555000	666000	LOW	2024-10-07T13:26:59.894Z	-29445000	-98.15	27-03-2020	27-03-2020	30000000		
54	1	Leasehold	Purpose Built Flat		2150	2300	2450	447000	496000	546000	MEDIUM	2024-10-07T13:26:59.894Z	46000	10.22222222	16-09-2022	16-09-2022	450000		
73	1	Leasehold	Purpose Built Flat	D	2800	3000	3250	619000	651000	684000	HIGH	2024-10-07T13:26:59.894Z	28000	4.494382022	31-10-2023	31-10-2023	623000	3.833333333	23000
73	1	Leasehold	Purpose Built Flat	D	2800	3000	3250	619000	651000	684000	HIGH	2024-10-07T13:26:59.894Z	28000	4.494382022	31-10-2023	23-03-2016	600000	155.3191489	365000
73	1	Leasehold	Purpose Built Flat	D	2800	3000	3250	619000	651000	684000	HIGH	2024-10-07T13:26:59.894Z	28000	4.494382022	31-10-2023	28-11-2005	235000	181.4371257	151500


By understanding above dataset give the Pyspark sql code for following question in one terminal

1.Find the postcode with the highest average property sale price
2. Find the property type with the highest average no. of bathrooms
3.Calculate the total no. of properties available in each country
4.Find the average percentage change in sale price (saleEstimation_valueChange. percentage Change) for each tenure type.



from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session
spark = SparkSession.builder \
    .appName("PropertyDataAnalysis") \
    .getOrCreate()

# Define the schema based on the dataset you've provided
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType

schema = StructType([
    StructField("floorAreaSqM", FloatType(), True),
    StructField("livingRooms", IntegerType(), True),
    StructField("tenure", StringType(), True),
    StructField("propertyType", StringType(), True),
    StructField("currentEnergyRating", StringType(), True),
    StructField("rentEstimate_lowerPrice", FloatType(), True),
    StructField("rentEstimate_currentPrice", FloatType(), True),
    StructField("rentEstimate_upperPrice", FloatType(), True),
    StructField("saleEstimate_lowerPrice", FloatType(), True),
    StructField("saleEstimate_currentPrice", FloatType(), True),
    StructField("saleEstimate_upperPrice", FloatType(), True),
    StructField("saleEstimate_confidenceLevel", StringType(), True),
    StructField("saleEstimate_ingestedAt", StringType(), True),
    StructField("saleEstimate_valueChange_numericChange", FloatType(), True),
    StructField("saleEstimate_valueChange_percentageChange", FloatType(), True),
    StructField("saleEstimate_valueChange_saleDate", DateType(), True),
    StructField("history_date", DateType(), True),
    StructField("history_price", FloatType(), True),
    StructField("history_percentageChange", FloatType(), True),
    StructField("history_numericChange", FloatType(), True),
])





exam/datload/Q1.ipynb

# Intialization

import os

import sys

​

os.environ["SPARK_HOME"] = "/home/talentum/spark"

os.environ["PYLIB"] = os.environ["SPARK_HOME"] + "/python/lib"

# In below two lines, use /usr/bin/python2.7 if you want to use Python 2

os.environ["PYSPARK_PYTHON"] = "/usr/bin/python3.6" 

os.environ["PYSPARK_DRIVER_PYTHON"] = "/usr/bin/python3"

sys.path.insert(0, os.environ["PYLIB"] +"/py4j-0.10.7-src.zip")

sys.path.insert(0, os.environ["PYLIB"] +"/pyspark.zip")

​

# NOTE: Whichever package you want mention here.

# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' 

# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'

os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'

# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'

#Entrypoint 2.x

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().getOrCreate()

​

# On yarn:

# spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().master("yarn").getOrCreate()

# specify .master("yarn")

​

sc = spark.sparkContext




# Load the dataset into a DataFrame
df = spark.read.csv("/path/to/your/dataset.csv", header=True, schema=schema)

# Register the DataFrame as a temporary view
df.createOrReplaceTempView("property_data")

# Question 1: Find the postcode with the highest average property sale price
# Note: This dataset doesn't include a postcode column, but if there was one, you could use the following query.
# For now, we'll use 'saleEstimate_currentPrice' to find the highest average sale price by property type.
query_1 = """
    SELECT propertyType, AVG(saleEstimate_currentPrice) AS avg_sale_price
    FROM property_data
    GROUP BY propertyType
    ORDER BY avg_sale_price DESC
    LIMIT 1
"""
result_1 = spark.sql(query_1)
result_1.show()

# Question 2: Find the property type with the highest average number of bathrooms
# Assumption: We will assume 'livingRooms' might be a proxy for bathrooms, as no direct 'bathrooms' column is present.
query_2 = """
    SELECT propertyType, AVG(livingRooms) AS avg_bathrooms
    FROM property_data
    GROUP BY propertyType
    ORDER BY avg_bathrooms DESC
    LIMIT 1
"""
result_2 = spark.sql(query_2)
result_2.show()

# Question 3: Calculate the total number of properties available in each country
# Note: The dataset doesn't have a direct country column. This query assumes you have a 'country' column.
# If the dataset includes a 'country' column, the query would look like this:
# query_3 = """
#     SELECT country, COUNT(*) AS total_properties
#     FROM property_data
#     GROUP BY country
# """

# For now, we will assume 'propertyType' represents different "types of countries."
query_3 = """
    SELECT propertyType, COUNT(*) AS total_properties
    FROM property_data
    GROUP BY propertyType
"""
result_3 = spark.sql(query_3)
result_3.show()

# Question 4: Find the average percentage change in sale price (saleEstimate_valueChange.percentageChange) for each tenure type
query_4 = """
    SELECT tenure, AVG(saleEstimate_valueChange_percentageChange) AS avg_percentage_change
    FROM property_data
    GROUP BY tenure
"""
result_4 = spark.sql(query_4)
result_4.show()











